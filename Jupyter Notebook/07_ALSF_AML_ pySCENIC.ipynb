{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pySCENIC  protocol\n",
    "\n",
    "https://github.com/aertslab/SCENICprotocol/blob/master/notebooks/PBMC10k_SCENIC-protocol-CLI.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import loompy as lp\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.settings.set_figure_params(dpi= 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.settings.verbosity = 3             # verbosity: errors (0), warnings (1), info (2), hints (3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.logging.print_header()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc._settings.ScanpyConfig( figdir='/oak/stanford/groups/cgawad/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/figures/',\n",
    "                          writedir='/oak/stanford/groups/cgawad/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/write/', \n",
    "                          cachedir='/oak/stanford/groups/cgawad/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/cache/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lp._version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the Cell ranger output folders\n",
    "file_load_path='/oak/stanford/groups/cgawad/home/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/Nova_fastqData/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HealthyBM1 = sc.read_10x_mtx(\n",
    "    file_load_path+'BM1-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "    gex_only=True,              # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HealthyBM2 = sc.read_10x_mtx(\n",
    "   file_load_path+'BM2-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "   gex_only=True,                  # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4363 = sc.read_10x_mtx(\n",
    "   file_load_path+'4363-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "    gex_only=True,                     # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4239 = sc.read_10x_mtx(\n",
    "    file_load_path+'4239-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "    gex_only=True,                    # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4035 = sc.read_10x_mtx(\n",
    "    file_load_path+'4035-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "    gex_only=True,                     # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4090 = sc.read_10x_mtx(\n",
    "    file_load_path+'4090-CSF-GEX/outs/filtered_feature_bc_matrix/',   # the directory with the `.mtx` file\n",
    "    gex_only=True,                     # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4102 = sc.read_10x_mtx(\n",
    "    file_load_path+'4102-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "    gex_only=True,                    # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4116 = sc.read_10x_mtx(\n",
    "  file_load_path+'4116-CSF-GEX/outs/filtered_feature_bc_matrix/',   # the directory with the `.mtx` file\n",
    "  gex_only=True,                   # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4127 = sc.read_10x_mtx(\n",
    "   file_load_path+'4127-CSF-GEX/outs/filtered_feature_bc_matrix/',   # the directory with the `.mtx` file\n",
    "   gex_only=True,                   # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4192 = sc.read_10x_mtx(\n",
    "    file_load_path+'4192-CSF-GEX/outs/filtered_feature_bc_matrix/',   # the directory with the `.mtx` file\n",
    "   gex_only=True,                    # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4232= sc.read_10x_mtx(\n",
    "    file_load_path+'4232-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "    gex_only=True,                     # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4264 = sc.read_10x_mtx(\n",
    "  file_load_path+'4264-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "  gex_only=True,                    # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4271 = sc.read_10x_mtx(\n",
    "    file_load_path+'4271-CSF-GEX/outs/filtered_feature_bc_matrix/',   # the directory with the `.mtx` file\n",
    "    gex_only=True,                    # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4304 = sc.read_10x_mtx(\n",
    "    file_load_path+'4304-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "    gex_only=True,                     # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4226 = sc.read_10x_mtx(\n",
    "    file_load_path+'4226-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "    gex_only=True,                     # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML948= sc.read_10x_mtx(\n",
    "   file_load_path+'948-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "    gex_only=True,                        # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML882= sc.read_10x_mtx(\n",
    "    file_load_path+'882-CSF-GEX/outs/filtered_feature_bc_matrix/',   # the directory with the `.mtx` file\n",
    "    gex_only=True,                         # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4068 = sc.read_10x_mtx(\n",
    "    file_load_path+'4068-CSF-GEX/outs/filtered_feature_bc_matrix/', # the directory with the `.mtx` file\n",
    "   gex_only=True,                         # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4062 = sc.read_10x_mtx(\n",
    "    file_load_path+'4062-CSF-GEX/outs/filtered_feature_bc_matrix/',   # the directory with the `.mtx` file\n",
    "    gex_only=True,                        # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4010 = sc.read_10x_mtx(\n",
    "   file_load_path+'4010-CSF-GEX/outs/filtered_feature_bc_matrix/',   # the directory with the `.mtx` file\n",
    "    gex_only=True,                     # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4000 = sc.read_10x_mtx(\n",
    "    file_load_path+'4000-CSF-GEX/outs/filtered_feature_bc_matrix/',   # the directory with the `.mtx` file\n",
    "    gex_only=True,                       # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML3492 = sc.read_10x_mtx(\n",
    "    file_load_path+'3492-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "    gex_only=True,                      # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML3371 = sc.read_10x_mtx(\n",
    "   file_load_path+'3371-CSF-GEX/outs/filtered_feature_bc_matrix/',   # the directory with the `.mtx` file\n",
    "    gex_only=True,                       # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML3210= sc.read_10x_mtx(\n",
    "    file_load_path+'3210-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "    gex_only=True,                       # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML3121= sc.read_10x_mtx(\n",
    "    file_load_path+'3121-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "    gex_only=True,                       # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML3082= sc.read_10x_mtx(\n",
    "    file_load_path+'3082-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "    gex_only=True,                      # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML3082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4048= sc.read_10x_mtx(\n",
    "   file_load_path+'4048-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "    gex_only=True,                       # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML1355= sc.read_10x_mtx(\n",
    "    file_load_path+'1355-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "    gex_only=True,                    # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML1355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML647= sc.read_10x_mtx(\n",
    "    file_load_path+'647-CSF-GEX/outs/filtered_feature_bc_matrix/', # the directory with the `.mtx` file\n",
    "     gex_only=True,                       # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML335= sc.read_10x_mtx(\n",
    "   file_load_path+'335-CSF-GEX/outs/filtered_feature_bc_matrix/',  # the directory with the `.mtx` file\n",
    "      gex_only=True,                       # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML335"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Normal Control Combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample_list=[HealthyBM1,\n",
    "             HealthyBM2,\n",
    "             AML335, \n",
    "             AML647,\n",
    "             AML882,\n",
    "             AML948,\n",
    "             AML1355,\n",
    "             AML3082,\n",
    "             AML3121,\n",
    "             AML3210,\n",
    "             AML3371,\n",
    "             AML3492,\n",
    "             AML4000, \n",
    "             AML4010,\n",
    "             AML4035,\n",
    "             AML4048,\n",
    "             AML4062,\n",
    "             AML4068,\n",
    "             AML4090,\n",
    "             AML4102,\n",
    "             AML4116,\n",
    "             AML4127,\n",
    "             AML4192,\n",
    "             AML4226,\n",
    "             AML4232,\n",
    "             AML4239,\n",
    "             AML4264,\n",
    "             AML4271,\n",
    "             AML4304,\n",
    "             AML4363]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single sample preprecessing for doublets detection.\n",
    "def scanpyPreprocessing_SS(adata):\n",
    "    adata.X = adata.X.astype('float64')\n",
    "    sc.pp.filter_cells(adata, min_genes=200)\n",
    "    sc.external.pp.scrublet(adata)\n",
    "    sc.external.pl.scrublet_score_distribution(adata)\n",
    "    sc.pl.violin(adata, ['doublet_score'],jitter=0.4, multi_panel=True)\n",
    "   # adata = adata[adata.obs.predicted_doublet==False, :]\n",
    "  \n",
    "    return(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in Sample_list:\n",
    "    scanpyPreprocessing_SS(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HealthyBM1  = HealthyBM1[HealthyBM1.obs.doublet_score<0.2, :]\n",
    "HealthyBM2  = HealthyBM2[HealthyBM2.obs.doublet_score<0.2, :]\n",
    "AML335  = AML335[AML335.obs.predicted_doublet==False, :]\n",
    "AML647  = AML647[AML647.obs.predicted_doublet==False, :]\n",
    "AML882  = AML882[AML882.obs.predicted_doublet==False, :]\n",
    "AML948  = AML948[AML948.obs.predicted_doublet==False, :]\n",
    "AML1355  = AML1355[AML1355.obs.predicted_doublet==False, :]\n",
    "AML3082  = AML3082[AML3082.obs.predicted_doublet==False, :]\n",
    "AML3121  = AML3121[AML3121.obs.predicted_doublet==False, :]\n",
    "AML3210  = AML3210[AML3210.obs.predicted_doublet==False, :]\n",
    "AML3371  = AML3371[AML3371.obs.predicted_doublet==False, :]\n",
    "AML3492  = AML3492[AML3492.obs.predicted_doublet==False, :]\n",
    "AML4000  = AML4000[AML4000.obs.predicted_doublet==False, :]\n",
    "AML4010  = AML4010[AML4010.obs.predicted_doublet==False, :]\n",
    "AML4035  = AML4035[AML4035.obs.predicted_doublet==False, :]\n",
    "AML4048  = AML4048[AML4048.obs.predicted_doublet==False, :]\n",
    "AML4062  = AML4062[AML4062.obs.predicted_doublet==False, :]\n",
    "AML4068  = AML4068[AML4068.obs.predicted_doublet==False, :]\n",
    "AML4090  = AML4090[AML4090.obs.predicted_doublet==False, :]\n",
    "AML4102  = AML4102[AML4102.obs.predicted_doublet==False, :]\n",
    "AML4116  = AML4116[AML4116.obs.predicted_doublet==False, :]\n",
    "AML4127  = AML4127[AML4127.obs.predicted_doublet==False, :]\n",
    "AML4192  = AML4192[AML4192.obs.predicted_doublet==False, :]\n",
    "AML4226  = AML4226[AML4226.obs.predicted_doublet==False, :]\n",
    "AML4232  = AML4232[AML4232.obs.predicted_doublet==False, :]\n",
    "AML4239  = AML4239[AML4239.obs.predicted_doublet==False, :]\n",
    "AML4264  = AML4264[AML4264.obs.predicted_doublet==False, :]\n",
    "AML4271  = AML4271[AML4271.obs.predicted_doublet==False, :]\n",
    "AML4304  = AML4304[AML4304.obs.predicted_doublet==False, :]\n",
    "AML4363  = AML4363[AML4363.obs.predicted_doublet==False, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML335  = AML335[AML335.obs.doublet_score<0.3, :]\n",
    "AML647  = AML647[AML647.obs.doublet_score<0.3, :]\n",
    "AML882  = AML882[AML882.obs.doublet_score<0.3, :]\n",
    "AML948  = AML948[AML948.obs.doublet_score<0.3, :]\n",
    "AML1355  = AML1355[AML1355.obs.doublet_score<0.3, :]\n",
    "AML3082  = AML3082[AML3082.obs.doublet_score<0.3, :]\n",
    "AML3121  = AML3121[AML3121.obs.doublet_score<0.3, :]\n",
    "AML3210  = AML3210[AML3210.obs.doublet_score<0.3, :]\n",
    "AML3371  = AML3371[AML3371.obs.doublet_score<0.3, :]\n",
    "AML3492  = AML3492[AML3492.obs.doublet_score<0.3, :]\n",
    "AML4000  = AML4000[AML4000.obs.doublet_score<0.3, :]\n",
    "AML4010  = AML4010[AML4010.obs.doublet_score<0.3, :]\n",
    "AML4035  = AML4035[AML4035.obs.doublet_score<0.3, :]\n",
    "AML4048  = AML4048[AML4048.obs.doublet_score<0.3, :]\n",
    "AML4062  = AML4062[AML4062.obs.doublet_score<0.3, :]\n",
    "AML4068  = AML4068[AML4068.obs.doublet_score<0.3, :]\n",
    "AML4090  = AML4090[AML4090.obs.doublet_score<0.3, :]\n",
    "AML4102  = AML4102[AML4102.obs.doublet_score<0.3, :]\n",
    "AML4116  = AML4116[AML4116.obs.doublet_score<0.3, :]\n",
    "AML4127  = AML4127[AML4127.obs.doublet_score<0.3, :]\n",
    "AML4192  = AML4192[AML4192.obs.doublet_score<0.3, :]\n",
    "AML4226  = AML4226[AML4226.obs.doublet_score<0.3, :]\n",
    "AML4232  = AML4232[AML4232.obs.doublet_score<0.3, :]\n",
    "AML4239  = AML4239[AML4239.obs.doublet_score<0.3, :]\n",
    "AML4264  = AML4264[AML4264.obs.doublet_score<0.3, :]\n",
    "AML4271  = AML4271[AML4271.obs.doublet_score<0.3, :]\n",
    "AML4304  = AML4304[AML4304.obs.doublet_score<0.3, :]\n",
    "AML4363  = AML4363[AML4363.obs.doublet_score<0.3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell_Num = 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4363_S = sc.pp.subsample(AML4363, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML4304_S = sc.pp.subsample(AML4304, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML4271_S = sc.pp.subsample(AML4271, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML4264_S = sc.pp.subsample(AML4264, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML4239_S = sc.pp.subsample(AML4239, n_obs = Cell_Num,random_state=0, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4232_S = sc.pp.subsample(AML4232, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML4226_S = sc.pp.subsample(AML4226, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML4192_S = sc.pp.subsample(AML4192, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML4127_S = sc.pp.subsample(AML4127, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "#AML4116_S = sc.pp.subsample(AML4116, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML4102_S = sc.pp.subsample(AML4102, n_obs = Cell_Num,random_state=0, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML4090_S = sc.pp.subsample(AML4090, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML4068_S = sc.pp.subsample(AML4068, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "#AML4062_S = sc.pp.subsample(AML4062, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML4035_S = sc.pp.subsample(AML4035, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "#AML4010_S = sc.pp.subsample(AML4010, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML4000_S = sc.pp.subsample(AML4000, n_obs = Cell_Num,random_state=0, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML3492_S = sc.pp.subsample(AML3492, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "#AML3371_S = sc.pp.subsample(AML3371, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML3210_S = sc.pp.subsample(AML3210, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML3121_S = sc.pp.subsample(AML3121, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML3082_S = sc.pp.subsample(AML3082, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML1355_S = sc.pp.subsample(AML1355, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "#AML948_S = sc.pp.subsample(AML948, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML882_S = sc.pp.subsample(AML882, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "#AML647_S = sc.pp.subsample(AML647, n_obs = Cell_Num,random_state=0, copy=True)\n",
    "AML335_S = sc.pp.subsample(AML335, n_obs = Cell_Num,random_state=0, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combo = HealthyBM1.concatenate(HealthyBM2,\n",
    "                               AML335_S, \n",
    "                               AML647,\n",
    "                               AML882_S,\n",
    "                               AML948,\n",
    "                               AML1355_S,\n",
    "                               AML3082_S,\n",
    "                               AML3121_S,\n",
    "                               AML3210_S,\n",
    "                               AML3371,\n",
    "                               AML3492_S,\n",
    "                               AML4000_S, \n",
    "                               AML4010,\n",
    "                               AML4035_S,\n",
    "                               AML4048,\n",
    "                               AML4062,\n",
    "                               AML4068_S,\n",
    "                               AML4090_S,\n",
    "                               AML4102_S,\n",
    "                               AML4116,\n",
    "                               AML4127_S,\n",
    "                               AML4192_S,\n",
    "                               AML4226_S,\n",
    "                               AML4232_S,\n",
    "                               AML4239_S,\n",
    "                               AML4264_S,\n",
    "                               AML4271_S,\n",
    "                               AML4304_S,\n",
    "                               AML4363_S,batch_key='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchDict = { \n",
    "'0':'0_HealthyBM1',\n",
    "'1':'0_HealthyBM2',\n",
    "'2':'CN',\n",
    "'3':'del7q',\n",
    "'4':'t(7;14)(q21;q32)',\n",
    "'5':'CN',\n",
    "'6':'PML/RARA',\n",
    "'7':'CBFB/MYH11',\n",
    "'8':'t(2;3)(p15;q26.2)',\n",
    "'9':'MLLr',\n",
    "'10':'Tri(15)',\n",
    "'11':'PML/RARA',\n",
    "'12':'MLLr',\n",
    "'13':'CN',\n",
    "'14':'MYB/GATA1',\n",
    "'15':'MLLr',\n",
    "'16':'CBFB/MYH11',\n",
    "'17':'CN',\n",
    "'18':'Tri(8)/MLLr',\n",
    "'19':'RUNX1/RUNX1T1',\n",
    "'20':'NUP98/NSD1',\n",
    "'21':'Tri(8)',\n",
    "'22':'Tri(8)/MLLr',\n",
    "'23':'CN',\n",
    "'24':'RUNX1/RUNX1T1',\n",
    "'25':'CBFB/MYH11',\n",
    "'26':'RUNX1/RUNX1T1',\n",
    "'27':'CBFB/MYH11',\n",
    "'28':'MLLr',\n",
    "'29':'BCR/ABL'\n",
    "            }\n",
    "\n",
    "Combo.obs['Cytogenetic'] = (\n",
    "    Combo.obs['batch']\n",
    "    .map(batchDict)\n",
    "    .astype('category')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchDict = { \n",
    "'0':'0_HealthyBM1',\n",
    "'1':'0_HealthyBM2',\n",
    "'2':'FLT3/ITD',\n",
    "'3':'del7q',\n",
    "'4':'t(7;14)(q21;q32)',\n",
    "'5':'FLT3/ITD',\n",
    "'6':'PML/RARA',\n",
    "'7':'CBFB/MYH11',\n",
    "'8':'t(2;3)(p15;q26.2)',\n",
    "'9':'MLLr',\n",
    "'10':'Tri(15)//FLT3/ITD',\n",
    "'11':'PML/RARA',\n",
    "'12':'MLLr',\n",
    "'13':'FLT3/ITD//NPM1c//IDH2-R140Q',\n",
    "'14':'MYB/GATA1//PIK3CA-E542K',\n",
    "'15':'MLLr//KRAS-G13D//ZFP36L2-fs',\n",
    "'16':'CBFB/MYH11//NRAS-Q61K//MET-P991S',\n",
    "'17':'FLT3/TKD',\n",
    "'18':'Tri(8)//MLLr//IDH2-R172K',\n",
    "'19':'RUNX1/RUNX1T1//TET2-C1271*//CSF3R-S783fs//JAK3-M511I//NCOR1-R933*',\n",
    "'20':'NUP98/NSD1//WT1-S364LfsTer//KRAS-G12D//USP37-Q484',\n",
    "'21':'Tri(8)',\n",
    "'22':'Tri(8)//MLLr//FLT3-S451F//SPI1-R231C//BCORL1-fs',\n",
    "'23':'NRAS-G12D//SOS1-R552K',\n",
    "'24':'RUNX1/RUNX1T1',\n",
    "'25':'CBFB/MYH11//KIT-D816V',\n",
    "'26':'RUNX1/RUNX1T1//NRAS-G13D//SMARCB1-R40Q',\n",
    "'27':'CBFB/MYH11//NRAS-G13D//KRAS-G13D//PTEN-R234P',\n",
    "'28':'MLLr//NRAS-Q61K',\n",
    "'29':'BCR/ABL//FLT3-D835(TKD)'\n",
    "            }\n",
    "\n",
    "Combo.obs['Genetic subtype'] = (\n",
    "    Combo.obs['batch']\n",
    "    .map(batchDict)\n",
    "    .astype('category')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchDict = { \n",
    "'0':'0_HealthyBM1',\n",
    "'1':'0_HealthyBM2',\n",
    "'2':'M5',\n",
    "'3':'M4',\n",
    "'4':'MPAL',\n",
    "'5':'M2',\n",
    "'6':'M3',\n",
    "'7':'M4Eo',\n",
    "'8':'MPAL',\n",
    "'9':'M5',\n",
    "'10':'M5',\n",
    "'11':'M3',\n",
    "'12':'M2',\n",
    "'13':'FLT3/ITD',\n",
    "'14':'M6',\n",
    "'15':'M5',\n",
    "'16':'M4Eo',\n",
    "'17':'M5',\n",
    "'18':'M1',\n",
    "'19':'M1',\n",
    "'20':'M1',\n",
    "'21':'Tri(8)',\n",
    "'22':'M5',\n",
    "'23':'M2',\n",
    "'24':'M2',\n",
    "'25':'M5',\n",
    "'26':'RUNX1/RUNX1T1',\n",
    "'27':'M4Eo',\n",
    "'28':'M2',\n",
    "'29':'MPAL'\n",
    "            }\n",
    "\n",
    "Combo.obs['FAB'] = (\n",
    "    Combo.obs['batch']\n",
    "    .map(batchDict)\n",
    "    .astype('category')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchDict = { \n",
    "'0':'0_HealthyBM1',\n",
    "'1':'0_HealthyBM2',\n",
    "'2':'AML335',\n",
    "'3':'AML647',\n",
    "'4':'AML882',\n",
    "'5':'AML948',\n",
    "'6':'AML1355',\n",
    "'7':'AML3082',\n",
    "'8':'AML3121',\n",
    "'9':'AML3210',\n",
    "'10':'AML3371',\n",
    "'11':'AML3492',\n",
    "'12':'AML4000',\n",
    "'13':'AML4010',\n",
    "'14':'AML4035',\n",
    "'15':'AML4048',\n",
    "'16':'AML4062',\n",
    "'17':'AML4068',\n",
    "'18':'AML4090',\n",
    "'19':'AML4102',\n",
    "'20':'AML4116',\n",
    "'21':'AML4127',\n",
    "'22':'AML4192',\n",
    "'23':'AML4226',\n",
    "'24':'AML4232',\n",
    "'25':'AML4239',\n",
    "'26':'AML4264',\n",
    "'27':'AML4271',\n",
    "'28':'AML4304',\n",
    "'29':'AML4363'\n",
    "            }\n",
    "\n",
    "Combo.obs['SampleID'] = (\n",
    "    Combo.obs['batch']\n",
    "    .map(batchDict)\n",
    "    .astype('category')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchDict = { \n",
    "'0':'0_HealthyBM',\n",
    "'1':'0_HealthyBM',\n",
    "'2':'1-10',\n",
    "'3':'>20',\n",
    "'4':'>10',\n",
    "'5':'1-10',\n",
    "'6':'1-10',\n",
    "'7':'1-10',\n",
    "'8':'>10',\n",
    "'9':'1-10',\n",
    "'10':'>10',\n",
    "'11':'>10',\n",
    "'12':'>10',\n",
    "'13':'>10',\n",
    "'14':'0-1',\n",
    "'15':'0-1',\n",
    "'16':'1-10',\n",
    "'17':'>20',\n",
    "'18':'>10',\n",
    "'19':'1-10',\n",
    "'20': '>10',\n",
    "'21':'>20',\n",
    "'22':'1-10',\n",
    "'23':'1-10',\n",
    "'24':'1-10',\n",
    "'25':'>10',\n",
    "'26':'>10',\n",
    "'27':'>10',\n",
    "'28':'1-10',\n",
    "'29':'>10',\n",
    "            }\n",
    "\n",
    "Combo.obs['Age'] = (\n",
    "    Combo.obs['batch']\n",
    "    .map(batchDict)\n",
    "    .astype('category')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchDict = { \n",
    "'0':'0_HealthyBM',\n",
    "'1':'0_HealthyBM',\n",
    "'2' :'Alive',\n",
    "'3' :'Deceased',\n",
    "'4' :'Deceased',\n",
    "'5' :'Alive',\n",
    "'6' :'Alive',\n",
    "'7' :'Alive',\n",
    "'8' :'Deceased',\n",
    "'9' :'Alive',\n",
    "'10' :'Deceased',\n",
    "'11' :'Alive',\n",
    "'12' :'Deceased',\n",
    "'13' :'Alive',\n",
    "'14' :'Alive',\n",
    "'15' :'Alive',\n",
    "'16' :'Alive',\n",
    "'17' :'Deceased',\n",
    "'18' :'Alive',\n",
    "'19' :'Alive',\n",
    "'20' :'Alive',\n",
    "'21' :'Alive',\n",
    "'22' :'Alive',\n",
    "'23' :'Alive',\n",
    "'24' :'Alive',\n",
    "'25' :'Alive',\n",
    "'26' :'Alive',\n",
    "'27' :'Deceased',\n",
    "'28' :'Deceased',\n",
    "'29' :'none',\n",
    "            }\n",
    "\n",
    "Combo.obs['Prognosis'] = (\n",
    "    Combo.obs['batch']\n",
    "    .map(batchDict)\n",
    "    .astype('category')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchDict = { \n",
    "'0':'0_HealthyBM',\n",
    "'1':'0_HealthyBM',\n",
    "'2':'False',\n",
    "'3':'False',\n",
    "'4':'none',\n",
    "'5':'True',\n",
    "'6':'False',\n",
    "'7':'True',\n",
    "'8':'True',\n",
    "'9':'False',\n",
    "'10':'True',\n",
    "'11':'True',\n",
    "'12':'False',\n",
    "'13':'False',\n",
    "'14':'False',\n",
    "'15':'False',\n",
    "'16':'False',\n",
    "'17':'True',\n",
    "'18':'False',\n",
    "'19':'False',\n",
    "'20':'False',\n",
    "'21':'False',\n",
    "'22':'False',\n",
    "'23':'False',\n",
    "'24':'False',\n",
    "'25':'True',\n",
    "'26':'False',\n",
    "'27':'none',\n",
    "'28':'True',\n",
    "'29':'none'\n",
    "            }\n",
    "\n",
    "Combo.obs['Relapsed'] = (\n",
    "    Combo.obs['batch']\n",
    "    .map(batchDict)\n",
    "    .astype('category')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchDict = { \n",
    "'0':'0_HealthyBM',\n",
    "'1':'0_HealthyBM',\n",
    "'2':'True',\n",
    "'3':'True',\n",
    "'4':'False',\n",
    "'5':'True',\n",
    "'6':'True',\n",
    "'7':'True',\n",
    "'8':'none',\n",
    "'9':'True',\n",
    "'10':'none',\n",
    "'11':'True',\n",
    "'12':'True',\n",
    "'13':'True',\n",
    "'14':'True',\n",
    "'15':'True',\n",
    "'16':'True',\n",
    "'17':'none',\n",
    "'18':'True',\n",
    "'19':'True',\n",
    "'20':'True',\n",
    "'21':'True',\n",
    "'22':'True',\n",
    "'23':'True',\n",
    "'24':'True',\n",
    "'25':'True',\n",
    "'26':'True',\n",
    "'27':'False',\n",
    "'28':'none',\n",
    "'29':'none',\n",
    "            }\n",
    "\n",
    "Combo.obs['Remission'] = (\n",
    "    Combo.obs['batch']\n",
    "    .map(batchDict)\n",
    "    .astype('category')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchDict = { \n",
    "'0':'HealthyBM',\n",
    "'1':'HealthyBM',\n",
    "'2':'AML',\n",
    "'3':'AML',\n",
    "'4':'AML',\n",
    "'5':'AML',\n",
    "'6':'AML',\n",
    "'7':'AML',\n",
    "'8':'AML',\n",
    "'9':'AML',\n",
    "'10':'AML',\n",
    "'11':'AML',\n",
    "'12':'AML',\n",
    "'13':'AML',\n",
    "'14':'AML',\n",
    "'15':'AML',\n",
    "'16':'AML',\n",
    "'17':'AML',\n",
    "'18':'AML',\n",
    "'19':'AML',\n",
    "'20':'AML',\n",
    "'21':'AML',\n",
    "'22':'AML',\n",
    "'23':'AML',\n",
    "'24':'AML',\n",
    "'25':'AML',\n",
    "'26':'AML',\n",
    "'27':'AML',\n",
    "'28':'AML',\n",
    "'29':'AML',\n",
    "            }\n",
    "\n",
    "Combo.obs['SampleType'] = (\n",
    "    Combo.obs['batch']\n",
    "    .map(batchDict)\n",
    "    .astype('category')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchDict = { \n",
    "'0':'HealthyBM',\n",
    "'1':'HealthyBM',\n",
    "'2':'AML',\n",
    "'3':'AML',\n",
    "'4':'AML',\n",
    "'5':'AML',\n",
    "'6':'AML',\n",
    "'7':'AML',\n",
    "'8':'AML',\n",
    "'9':'AML',\n",
    "'10':'AML',\n",
    "'11':'AML',\n",
    "'12':'AML',\n",
    "'13':'AML',\n",
    "'14':'AML',\n",
    "'15':'AML',\n",
    "'16':'AML',\n",
    "'17':'AML',\n",
    "'18':'AML',\n",
    "'19':'AML',\n",
    "'20':'AML',\n",
    "'21':'AML',\n",
    "'22':'AML',\n",
    "'23':'AML',\n",
    "'24':'AML',\n",
    "'25':'AML',\n",
    "'26':'AML',\n",
    "'27':'AML',\n",
    "'28':'AML',\n",
    "'29':'AML',\n",
    "            }\n",
    "\n",
    "Combo.obs['SampleType'] = (\n",
    "    Combo.obs['batch']\n",
    "    .map(batchDict)\n",
    "    .astype('category')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchDict = { \n",
    "'0':'0_HealthyBM1',\n",
    "'1':'0_HealthyBM2',\n",
    "'2':'AML335-M5a-FLT3/ITD',\n",
    "'3':'AML647-M4-del7q',\n",
    "'4':'AML882-MPAL-t(7;14)(q21;q32)',\n",
    "'5':'AML948-M2-FLT3/ITD',\n",
    "'6':'AML1355-M3-PML/RARA',\n",
    "'7':'AML3082-M4Eo-CBFB/MYH11',\n",
    "'8':'AML3121-MPAL-t(2;3)(p15;q26.2)',\n",
    "'9':'AML3210-M5a-MLLr',\n",
    "'10':'AML3371-M5b-FLT3/ITD-Tri(15)',\n",
    "'11':'AML3492-M3-PML/RARA',\n",
    "'12':'AML4000-M2-MLLr',\n",
    "'13':'AML4010-FLT3/ITD',\n",
    "'14':'AML4035-M6-MYB/GATA1',\n",
    "'15':'AML4048-M5-MLLr',\n",
    "'16':'AML4062-CBFB/MYH11',\n",
    "'17':'AML4068-M5b-FLT3/TKD',\n",
    "'18':'AML4090-M1-Tri(8)/MLLr',\n",
    "'19':'AML4102-M1-RUNX1/RUNX1T1',\n",
    "'20':'AML4116-M1-NUP98/NSD1',\n",
    "'21':'AML4127-Tri(8)',\n",
    "'22':'AML4192-M5a-Tri(8)/MLLr',\n",
    "'23':'AML4226-M2-NRAS(G12D)',\n",
    "'24':'AML4232-M2-RUNX1/RUNX1T1',\n",
    "'25':'AML4239-M5a-CBFB/MYH11',\n",
    "'26':'AML4264-RUNX1/RUNX1T1',\n",
    "'27':'AML4271-CBFB/MYH11',\n",
    "'28':'AML4304-M2-MLLr',\n",
    "'29':'AML4363-MPAL-BCR/ABL'\n",
    "            \n",
    "            \n",
    "}\n",
    "\n",
    "Combo.obs['Sample'] = (\n",
    "    Combo.obs['batch']\n",
    "    .map(batchDict)\n",
    "    .astype('category')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna = Combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd = Combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combo =Scope_annd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='python_3.8_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_anndata_path = \"/oak/stanford/groups/cgawad/home/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/H5AD/Total_30_anndata_scope_%s.h5ad\"%name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_loom_path_unfilt = \"/oak/stanford/groups/cgawad/home/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/H5AD/Total_30_unfiltered_%s.loom\" %name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_loom_path_scenic = \"/oak/stanford/groups/cgawad/home/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/H5AD/Total_30_filtered_scenic_%s.loom\"%name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to pyscenic output\n",
    "f_pyscenic_output = \"/oak/stanford/groups/cgawad/home/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/H5AD/pyscenic_30_output_%s.loom\"%name\n",
    "\n",
    "# loom output, generated from a combination of Scanpy and pySCENIC results:\n",
    "f_final_loom = '/oak/stanford/groups/cgawad/home/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/H5AD/Total_30_scenic_integrated-output_%s.loom'%name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_attrs = { \n",
    "    \"Gene\": np.array(rna.var.index) ,\n",
    "   \n",
    "}\n",
    "col_attrs = { \n",
    "    \"CellID\":  np.array(rna.obs.index) ,\n",
    "    \"Sample\": np.array(rna.obs.Sample),\n",
    "    \"nGene\": np.array( np.sum(rna.X.transpose()>0 , axis=0)).flatten() ,\n",
    "    \"nUMI\": np.array( np.sum(rna.X.transpose() , axis=0)).flatten() ,\n",
    "    \n",
    "}\n",
    "\n",
    "lp.create( f_loom_path_unfilt, rna.X.transpose(), row_attrs, col_attrs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd=rna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCountsPerGene = np.sum(Scope_annd.X, axis=0)\n",
    "nCellsPerGene = np.sum(Scope_annd.X>0, axis=0)\n",
    "\n",
    "# Show info\n",
    "print(\"Number of counts (in the dataset units) per gene:\", nCountsPerGene.min(), \" - \" ,nCountsPerGene.max())\n",
    "print(\"Number of cells in which each gene is detected:\", nCellsPerGene.min(), \" - \" ,nCellsPerGene.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCells=Scope_annd.X.shape[0]\n",
    "\n",
    "# pySCENIC thresholds\n",
    "minCountsPerGene=3*.01*nCells # 3 counts in 1% of cells\n",
    "print(\"minCountsPerGene: \", minCountsPerGene)\n",
    "\n",
    "minSamples=.01*nCells # 1% of cells\n",
    "print(\"minSamples: \", minSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd.var['mt'] = Scope_annd.var_names.str.startswith('MT-')\n",
    "sc.pp.calculate_qc_metrics(Scope_annd, qc_vars=['mt'], \n",
    "                           percent_top=None, \n",
    "                           log1p=False, \n",
    "                           inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply compute the number of genes per cell (computers 'n_genes' column)\n",
    "sc.pp.filter_cells(Scope_annd, min_genes=200)\n",
    "# mito and genes/counts cuts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4), dpi=150, sharey=True)\n",
    "\n",
    "x = Scope_annd.obs['n_genes']\n",
    "x_lowerbound = 1500\n",
    "x_upperbound = 3000\n",
    "nbins=100\n",
    "\n",
    "sns.histplot(x, ax=ax1, bins=nbins,kde=True, stat=\"density\")\n",
    "sns.histplot(x, ax=ax2, bins=nbins,kde=True,stat=\"density\")\n",
    "sns.histplot(x, ax=ax3, bins=nbins,kde=True,stat=\"density\")\n",
    "\n",
    "ax2.set_xlim(0,x_lowerbound)\n",
    "ax3.set_xlim(x_upperbound, Scope_annd.obs['n_genes'].max() )\n",
    "\n",
    "for ax in (ax1,ax2,ax3): \n",
    "    ax.set_xlabel('')\n",
    "\n",
    "ax1.title.set_text('n_genes')\n",
    "ax2.title.set_text('n_genes, lower bound')\n",
    "ax3.title.set_text('n_genes, upper bound')\n",
    "\n",
    "fig.text(-0.01, 0.5, 'Frequency', ha='center', va='center', rotation='vertical', size='x-large')\n",
    "fig.text(0.5, 0.0, 'Genes expressed per cell', ha='center', va='center', size='x-large')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('filtering_panel_n_genes.pdf', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4), dpi=150, sharey=True)\n",
    "\n",
    "x = Scope_annd.obs['pct_counts_mt']\n",
    "x_lowerbound = [0.0, 0.07 ]\n",
    "x_upperbound = [ 0.10, 0.3 ]\n",
    "nbins=100\n",
    "\n",
    "sns.histplot(x, ax=ax1, bins=nbins)\n",
    "sns.histplot(x, ax=ax2, bins=int(nbins/(x_lowerbound[1]-x_lowerbound[0])) )\n",
    "sns.histplot(x, ax=ax3, bins=int(nbins/(x_upperbound[1]-x_upperbound[0])) )\n",
    "\n",
    "ax2.set_xlim(x_lowerbound[0], x_lowerbound[1])\n",
    "ax3.set_xlim(x_upperbound[0], x_upperbound[1] )\n",
    "for ax in (ax1,ax2,ax3): \n",
    "  ax.set_xlabel('')\n",
    "\n",
    "ax1.title.set_text('pct_counts_mt')\n",
    "ax2.title.set_text('pct_counts_mt, lower bound')\n",
    "ax3.title.set_text('pct_counts_mt, upper bound')\n",
    "\n",
    "fig.text(-0.01, 0.5, 'Frequency', ha='center', va='center', rotation='vertical', size='x-large')\n",
    "fig.text(0.5, 0.0, 'Mitochondrial read fraction per cell', ha='center', va='center', size='x-large')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('filtering_panel_mt.pdf', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4), dpi=150, sharey=False)\n",
    "\n",
    "sns.distplot( Scope_annd.obs['n_genes'], ax=ax1, norm_hist=True, bins=100)\n",
    "sns.distplot( Scope_annd.obs['total_counts'], ax=ax2, norm_hist=True, bins=100)\n",
    "sns.distplot( Scope_annd.obs['pct_counts_mt'], ax=ax3, norm_hist=True, bins=100)\n",
    "\n",
    "ax1.title.set_text('Number of genes expressed per cell')\n",
    "ax2.title.set_text('Counts per cell')\n",
    "ax3.title.set_text('Mitochondrial read fraction per cell')\n",
    "\n",
    "fig.text(-0.01, 0.5, 'Frequency', ha='center', va='center', rotation='vertical', size='x-large')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('filtering_panel_prefilter.pdf', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(Scope_annd, ['n_genes', 'total_counts', 'pct_counts_mt'],\n",
    "    jitter=0.4, multi_panel=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.scatter(Scope_annd, x='total_counts', y='n_genes', color='pct_counts_mt')\n",
    "#'total_counts','n_genes_by_counts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.filter_genes(Scope_annd, min_cells=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd = Scope_annd[Scope_annd.obs['total_counts'] < 40000, :]\n",
    "Scope_annd = Scope_annd[Scope_annd.obs['pct_counts_mt'] < 10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4), dpi=150, sharey=False)\n",
    "\n",
    "Scope_annd.obs['n_genes']\n",
    "\n",
    "sns.distplot( Scope_annd.obs['n_genes'], ax=ax1, norm_hist=True, bins=100)\n",
    "sns.distplot( Scope_annd.obs['total_counts'], ax=ax2, norm_hist=True, bins=100)\n",
    "sns.distplot( Scope_annd.obs['pct_counts_mt'], ax=ax3, norm_hist=True, bins=100)\n",
    "\n",
    "ax1.title.set_text('Number of genes expressed per cell')\n",
    "ax2.title.set_text('Counts per cell')\n",
    "ax3.title.set_text('Mitochondrial read fraction per cell')\n",
    "\n",
    "fig.text(-0.01, 0.5, 'Frequency', ha='center', va='center', rotation='vertical', size='x-large')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('filtering_panel_postfilter.pdf', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(Scope_annd, ['n_genes', 'total_counts', 'pct_counts_mt'],\n",
    "    jitter=0.4, multi_panel=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_attrs = {\n",
    "    \"Gene\": np.array(Scope_annd.var_names) ,\n",
    "}\n",
    "col_attrs = {\n",
    "    \"CellID\": np.array(Scope_annd.obs_names) ,\n",
    "    \"Sample\": np.array(Scope_annd.obs.Sample),\n",
    "    \"nGene\": np.array( np.sum(Scope_annd.X.transpose()>0 , axis=0)).flatten() ,\n",
    "    \"nUMI\": np.array( np.sum(Scope_annd.X.transpose() , axis=0)).flatten() ,\n",
    "}\n",
    "lp.create(f_loom_path_scenic, Scope_annd.X.transpose(), row_attrs, col_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_genes = Scope_annd.var_names.str.startswith('MT-')\n",
    "#malat1= Scope_annd.var_names.str.startswith('MALAT1')\n",
    "ribo_genes =  Scope_annd.var_names.str.startswith((\"RPS\",\"RPL\"))\n",
    "hb_genes = Scope_annd.var_names.str.contains('^HB[^(P)]')\n",
    "Scope_annd.obs['percent_ribo'] = np.sum(\n",
    "     Scope_annd[:, ribo_genes].X, axis=1).A1 / np.sum( Scope_annd.X, axis=1).A1\n",
    "\n",
    "\n",
    "remove = np.add(mito_genes,ribo_genes)\n",
    "remove = np.add(remove,hb_genes)\n",
    "remove = np.add(remove,malat1)\n",
    "\n",
    "keep = np.invert(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd = Scope_annd[:,keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total-count normalize (library-size correct) to 10,000 reads/cell\n",
    "sc.pp.normalize_total(Scope_annd, target_sum=1e4)\n",
    "# log transform the data.\n",
    "sc.pp.log1p(Scope_annd)\n",
    "\n",
    "Scope_annd.raw = Scope_annd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify highly variable genes.\n",
    "sc.pp.highly_variable_genes(Scope_annd, \n",
    "                           min_mean=0.0125, \n",
    "                           max_mean=3, \n",
    "                           min_disp=0.5,\n",
    "                            batch_key='batch'\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only highly variable genes:\n",
    "Scope_annd = Scope_annd[:, Scope_annd.var['highly_variable']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regress out total counts per cell and the percentage of mitochondrial genes expressed\n",
    "#sc.pp.regress_out(Scope_annd, ['total_counts','n_genes_by_counts'] ) #, n_jobs=args.threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale each gene to unit variance, clip values exceeding SD 10.\n",
    "sc.pp.scale(Scope_annd, max_value=10)\n",
    "\n",
    "# update the anndata file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " sc.pp.combat(Scope_annd, key='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.pca(Scope_annd, svd_solver='arpack',n_comps=50)\n",
    "sc.pl.pca_variance_ratio(Scope_annd, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd.write(f_anndata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neighborhood graph of cells (determine optimal number of PCs here)\n",
    "sc.pp.neighbors(Scope_annd, n_neighbors=50, n_pcs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# compute UMAP\n",
    "sc.tl.umap(Scope_annd,min_dist=0.5,spread=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.leiden(Scope_annd,resolution=6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(Scope_annd, color=['leiden','Sample'],wspace=0.5,\n",
    "           #save='AML_30_sample_leiden_batchName_%s.png'%name\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(Scope_annd, color=['batch','Cell_Type'],wspace=0.5\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.tl.tsne(Scope_annd,n_pcs=30,n_jobs=24) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(Scope_annd, color=['leiden','Sample']\n",
    "           #save='AML_30_sample_leiden_batchName.png' \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(Scope_annd, color=['Cell_Type','lineage'],wspace=0.6,\n",
    "           #save='AML_30_sample_leiden_batchName.png' \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(Scope_annd, color=['Cell_Type_Sample'],wspace=0.8,\n",
    "           #save='AML_30_sample_leiden_batchName.png' \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.louvain(Scope_annd,resolution=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(Scope_annd, color=[   \n",
    "                               'PTPRC','AVP','CD34','CD38',#HSC CD34+CD38-SPINK2+AVP-IL3RA-IL7R-\n",
    "                               'IL3RA','FLT3',#myeloid progenitors\n",
    "                               'VCAN','CCR2','CD14','CD68','HES4','FCGR3A','KLF4','CD1C',#Monocytes precursor\n",
    "                               'ADGRE1','FCGR1A','CCR2','MARCO','ITGAM',# Macrophage\n",
    "                               'MPO','AZU1',# Graunulocytes precursor\n",
    "                               #'CEACAM8',# Neutrophils\n",
    "                               'GATA1','KLF1','TPSAB1',# Erythrocytes precursor\n",
    "                               'MS4A3',# Mast cells and Basophlis\n",
    "                               'GYPA','GYPB','PF4',# Erythrocytes\n",
    "                               'CD7','CD3D','CD4','CD8A',# T cells\n",
    "                               'KLRB1','GNLY','GZMK',# NK cells\n",
    "                               'IL7R','CD19','MME','EBF1','PAX5','MS4A1','CD22',# B cells\n",
    "                                'IGHA2','TNFRSF17',#plasma B cell\n",
    "                               'PCNA','MKI67',#cell cycle\n",
    "                               'ITGA6',\n",
    "                               'RUNX1T1',\n",
    "'GTSF1',\n",
    "'CD96',\n",
    "'MS4A4E',\n",
    "'ATP8B4',\n",
    "'HGF',\n",
    "'IFI6',\n",
    "'CCNA1',\n",
    "'MSLN',\n",
    "    'AC244502.1',\n",
    "'AC125603.2',\n",
    "    'MECOM','SLC24A3', \n",
    "                         \n",
    "                                 ], cmap='Reds',\n",
    "           save='_AML_30_Total_pyscenic.MarkerGenes_%s.png'%name\n",
    "          )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leiden_lis=pd.DataFrame(Scope_annd.obs['leiden'])['leiden'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leiden_lis.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marker genes for each lineage\n",
    "for group in leiden_lis:\n",
    "    \n",
    "    sc.pl.umap(Scope_annd, color=['leiden'],groups=group,frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd_leiden2manCT = {\n",
    "   \n",
    "  '0': 'AML',\n",
    "  '1': 'Naive CD4T',\n",
    "  '2': 'AML','3': 'AML','4': 'AML','5':'CD14_Monocyte', \n",
    "  '6':'AML', \n",
    "  '7': 'AML',\n",
    "  '8':'AML-CellCycle',  '9': 'CD14_Monocyte', '10':'AML','11': 'AML','12':'AML','13': 'AML','14': 'AML','15': 'AML',\n",
    "  '16': 'AML',\n",
    "  '17': 'AML-CD14',\n",
    "  '18': 'AML','19': 'AML','20': 'AML',\n",
    "\n",
    "  '21':'Naive CD8T',\n",
    "  '22': 'AML',\n",
    "  '23': 'AML-CellCycle',\n",
    "  '24':'AML','25': 'CD20+B','26': 'AML', \n",
    "  '27': 'AML-mDC',\n",
    "  '28': 'AML','29': 'NK',\n",
    "  '30': 'AML',\n",
    "  '31': 'AML',\n",
    "  '32': 'AML',\n",
    "  '33': 'AML-CellCycle',\n",
    "  '34': 'AML',\n",
    "  '35': 'AML-CD4T','36': 'AML-CTL',\n",
    "  '37': 'AML',\n",
    "  '38': 'AML',\n",
    "  '39': 'AML', '40': 'AML','41':'AML','42': 'AML-CD14','43': 'AML-Ery','44': 'AML',\n",
    "  '45': 'CTL', \n",
    "  '46': 'Erythrocytes',\n",
    "  '47': 'AML','48':'Myeloid Pro','49': 'AML', '50': 'AML','51': 'AML-Ery',\n",
    "  '52': 'AML','53': 'AML','54': 'AML-CD14','55': 'AML','56': 'AML',\n",
    "  '57': 'AML-CellCycle','58': 'AML','59': 'AML','60': 'AML','61': 'GZMB+CD8T',\n",
    "  '62': 'AML',\n",
    "  '63': 'MEM_CD8T',\n",
    "  '64':'AML','65': 'AML','66': 'AML',\n",
    "  '67': 'AML','68': 'ProB',\n",
    "  '69': 'mDC',\n",
    "  '70': 'AML',#AML4127\n",
    "  '71': 'PreB','72': 'CD16_Monocyte',\n",
    "  '73': 'HSPC', '74': 'AML','75': 'AML','76': 'AML-Ery','77': 'AML-CD8T',\n",
    "  '78': 'AML-Ery','79':'AML-B', '80': 'AML',\n",
    "  '81': 'AML-Ery','82':'pDC',\n",
    "  '83': 'AML-Ery','84': 'AML-NK','85':  'PlasmaB','86': 'AML','87': 'AML','88': 'AML-Ery',\n",
    "  '89': 'AML-CTL','90': 'AML-B',\n",
    "  '91': 'AML',\n",
    "  '92': 'AML','93':'AML', '94': 'AML-Ery','95': 'AML',\n",
    "    '96':'AML',\n",
    "    '97':'CD34+ProB',\n",
    "    '98':'Macrophage',\n",
    "    '99':'AML',\n",
    "    '100':'AML',\n",
    "    '101':'AML'\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "}\n",
    "Scope_annd.obs['Cell_Type'] = (\n",
    "   Scope_annd.obs['leiden']\n",
    "    .map(Scope_annd_leiden2manCT)\n",
    "    .astype('category')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd_leiden2manCT = {\n",
    "   \n",
    " '0': 'AML',\n",
    "  '1': 'T',\n",
    "  '2': 'AML','3': 'AML','4': 'AML','5':'Monocyte', \n",
    "  '6':'AML', \n",
    "  '7': 'AML',\n",
    "  '8':'AML',  '9': 'Monocyte', '10':'AML','11': 'AML','12':'AML','13': 'AML','14': 'AML','15': 'AML',\n",
    "  '16': 'AML',\n",
    "  '17': 'AML-Mono',\n",
    "  '18': 'AML','19': 'AML','20': 'AML',\n",
    "\n",
    "  '21':'T',\n",
    "  '22': 'AML',\n",
    "  '23': 'AML',\n",
    "  '24':'AML','25': 'B','26': 'AML', \n",
    "  '27': 'AML-Mono',\n",
    "  '28': 'AML','29': 'NK',\n",
    "  '30': 'AML',\n",
    "  '31': 'AML',\n",
    "  '32': 'AML',\n",
    "  '33': 'AML',\n",
    "  '34': 'AML',\n",
    "  '35': 'AML-T','36': 'AML-T',\n",
    "  '37': 'AML',\n",
    "  '38': 'AML',\n",
    "  '39': 'AML', '40': 'AML','41':'AML','42': 'AML-Mono','43': 'AML-Ery','44': 'AML',\n",
    "  '45': 'T', \n",
    "  '46': 'Erythrocytes',\n",
    "  '47': 'AML','48':'Myeloid Pro','49': 'AML', '50': 'AML','51': 'AML-Ery',\n",
    "  '52': 'AML','53': 'AML','54': 'AML-Mono','55': 'AML','56': 'AML',\n",
    "  '57': 'AML','58': 'AML','59': 'AML','60': 'AML','61': 'T',\n",
    "  '62': 'AML',\n",
    "  '63': 'T',\n",
    "  '64':'AML','65': 'AML','66': 'AML',\n",
    "  '67': 'AML','68': 'B',\n",
    "  '69': 'Monocyte',\n",
    "  '70': 'AML',\n",
    "  '71': 'B','72': 'Monocyte',\n",
    "  '73': 'HSPC', '74': 'AML','75': 'AML','76': 'AML-Ery','77': 'AML-T',\n",
    "  '78': 'AML-Ery','79':'AML-B', '80': 'AML',\n",
    "  '81': 'AML-Ery','82':'Monocyte',\n",
    "  '83': 'AML-Ery','84': 'AML-NK','85':  'PlasmaB','86': 'AML','87': 'AML','88': 'AML-Ery',\n",
    "  '89': 'AML-T','90': 'AML-B',\n",
    "  '91': 'AML',\n",
    "  '92': 'AML','93':'AML', '94': 'AML-Ery','95': 'AML',\n",
    "    '96':'AML',\n",
    "    '97':'B',\n",
    "    '98':'Monocyte',\n",
    "    '99':'AML',\n",
    "    '100':'AML',\n",
    "    '101':'AML'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "}\n",
    "Scope_annd.obs['lineage'] = (\n",
    "   Scope_annd.obs['leiden']\n",
    "    .map(Scope_annd_leiden2manCT)\n",
    "    .astype('category')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd.write(f_anndata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(Scope_annd, color=[ 'Cell_Type','lineage'], wspace=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd = sc.read_h5ad(f_anndata_path)\n",
    "Scope_annd.uns['log1p'][\"base\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='/oak/stanford/groups/cgawad/home/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/H5AD/'\n",
    "Combo=sc.read_h5ad(file_path+ \"ALSF_AML_Combo_3500_with_raw_count.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1=Combo.obs['PAC_anno']\n",
    "df_SCENIC=Scope_annd.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_SCENIC.merge(df_1, on='index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd.obs=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(Scope_annd, color=['PAC_anno'],\n",
    "          #save=\"_ALSF_AML_pyscenic_LSC_Cell_Type_set2.pdf\"\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd.write(f_anndata_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCENIC steps\n",
    "STEP 1: Gene regulatory network inference, and generation of co-expression modules\n",
    "\n",
    "Phase Ia: GRN inference using the GRNBoost2 algorithm\n",
    "\n",
    "For this step the CLI version of SCENIC is used. This step can be deployed on an High Performance Computing system. We use the counts matrix (without log transformation or further processing) from the loom file we wrote earlier. Output: List of adjacencies between a TF and its targets stored in ADJACENCIES_FNAME."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run it in command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcription factors list\n",
    "f_tfs = \"/ddn1/vol1/staging/leuven/stg_00002/lcb/cflerin/resources/allTFs_hg38.txt\" # human\n",
    "# f_tfs = \"/ddn1/vol1/staging/leuven/stg_00002/lcb/cflerin/resources/allTFs_dmel.txt\" # drosophila\n",
    "# f_tfs = \"/ddn1/vol1/staging/leuven/stg_00002/lcb/cflerin/resources/allTFs_mm.txt\"   # mouse\n",
    "# tf_names = load_tf_names( f_tfs )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyscenic grn {f_loom_path_scenic} {f_tfs} -o adj.csv --num_workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacencies = pd.read_csv(\"adj-28.tsv\", index_col=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacencies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2-3: Regulon prediction aka cisTarget from CLI\n",
    "\n",
    "For this step the CLI version of SCENIC is used. This step can be deployed on an High Performance Computing system.\n",
    "\n",
    "Output: List of adjacencies between a TF and its targets stored in MOTIFS_FNAME.\n",
    "\n",
    "locations for ranking databases, and motif annotations:\n",
    "\n",
    "Run it in command line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# ranking databases\n",
    "f_db_glob = \"/oak/stanford/groups/cgawad/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/pyscenic/*feather\"\n",
    "f_db_names = ' '.join( glob.glob(f_db_glob) )\n",
    "\n",
    "# motif databases\n",
    "f_motif= \"/oak/stanford/groups/cgawad/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/pyscenic/motifs.v9.nr.hgnc.tbl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_db_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_db_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_loom_path_scenic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyscenic ctx  \\\n",
    "/oak/stanford/groups/cgawad/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/adj_LSC__py3.8.csv \\\n",
    "     {f_db_names} \\\n",
    "    --annotations_fname {f_motif} \\\n",
    "    --expression_mtx_fname {f_loom_path_scenic} \\\n",
    "    --output reg-py_LSC_new.csv \\\n",
    "    --mask_dropouts \\\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Cellular enrichment (aka AUCell) from CLI\n",
    "\n",
    "It is important to check that most cells have a substantial fraction of expressed/detected genes in the calculation of the AUC. The following histogram gives an idea of the distribution and allows selection of an appropriate threshold. In this plot, a few thresholds are highlighted, with the number of genes selected shown in red text and the corresponding percentile in parentheses). See the relevant section in the R tutorial for more information.\n",
    "\n",
    "By using the default setting for --auc_threshold of 0.05, we see that 1192 genes are selected for the rankings based on the plot below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/aertslab/pySCENIC/issues/350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scope_annd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nGenesDetectedPerCellbefore = np.sum(Scope_annd.X>0, axis=1)\n",
    "nGenesDetectedPerCell = pd.Series(nGenesDetectedPerCellbefore)\n",
    "percentiles = nGenesDetectedPerCell.quantile([.01, .05, .10, .50, .80,.90,1])\n",
    "print(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run in command line\n",
    "! pyscenic aucell \\\n",
    "    {f_loom_path_scenic} \\\n",
    "    /oak/stanford/groups/cgawad/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/reg-py_LSC_new.csv \\\n",
    "    --output /oak/stanford/groups/cgawad/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/H5AD/pyscenic_LSC_output.loom \\\n",
    "    --auc_threshold 0.05 \\\n",
    "    --num_workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pyscenic_output=\"/oak/stanford/groups/cgawad/home/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/H5AD/pyscenic_LSC_output.loom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_loom_path_scenic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of SCENIC's AUC matrix\n",
    "First, load the relevant data from the loom we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import zlib\n",
    "import base64\n",
    "\n",
    "# collect SCENIC AUCell output\n",
    "lf = lp.connect(f_pyscenic_output, mode='r+', validate=False )\n",
    "auc_mtx = pd.DataFrame(lf.ca.RegulonsAUC, index=lf.ca.CellID)\n",
    "lf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "# UMAP\n",
    "runUmap = umap.UMAP(n_neighbors=15, min_dist=0.4, metric='correlation').fit_transform\n",
    "dr_umap = runUmap( auc_mtx )\n",
    "pd.DataFrame(dr_umap, columns=['X', 'Y'], index=auc_mtx.index).to_csv( \"scenic_umap.txt\", sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tSNE\n",
    "tsne = TSNE(n_jobs=20,init='pca')\n",
    "dr_tsne = tsne.fit_transform(auc_mtx)\n",
    "pd.DataFrame(dr_tsne, columns=['X', 'Y'], \n",
    "             index=auc_mtx.index).to_csv( \"scenic_tsne.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenic output\n",
    "lf = lp.connect(f_pyscenic_output, mode='r+', validate=False )\n",
    "meta = json.loads(zlib.decompress(base64.b64decode(lf.attrs.MetaData )))\n",
    "exprMat = pd.DataFrame( lf[:,:], index=lf.ra.Gene, columns=lf.ca.CellID)\n",
    "auc_mtx = pd.DataFrame( lf.ca.RegulonsAUC, index=lf.ca.CellID)\n",
    "regulons = lf.ra.Regulons\n",
    "dr_umap = pd.read_csv( 'scenic_umap.txt', sep='\\t', header=0, index_col=0 )\n",
    "dr_tsne = pd.read_csv( 'scenic_tsne.txt', sep='\\t', header=0, index_col=0 )\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_mtx.columns = auc_mtx.columns.str.replace('\\(','_(')\n",
    "regulons.dtype.names = tuple( [ x.replace(\"(\",\"_(\") for x in regulons.dtype.names ] )\n",
    "# regulon thresholds\n",
    "rt = meta['regulonThresholds']\n",
    "for i,x in enumerate(rt):\n",
    "    tmp = x.get('regulon').replace(\"(\",\"_(\")\n",
    "    x.update( {'regulon': tmp} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsneDF = pd.DataFrame(Scope_annd.obsm['X_tsne'], columns=['_X', '_Y'])\n",
    "\n",
    "Embeddings_X = pd.DataFrame(index=lf.ca.CellID )\n",
    "Embeddings_X = pd.concat( [\n",
    "        pd.DataFrame(Scope_annd.obsm['X_umap'],index=Scope_annd.obs.index)[0] ,\n",
    "        pd.DataFrame(Scope_annd.obsm['X_pca'],index=Scope_annd.obs.index)[0] ,\n",
    "        dr_tsne['X'] ,\n",
    "        dr_umap['X']\n",
    "    ], sort=False, axis=1, join='outer' )\n",
    "Embeddings_X.columns = ['1','2','3','4']\n",
    "\n",
    "Embeddings_Y = pd.DataFrame( index=lf.ca.CellID )\n",
    "Embeddings_Y = pd.concat( [\n",
    "        pd.DataFrame(Scope_annd.obsm['X_umap'],index=Scope_annd.obs.index)[1] ,\n",
    "        pd.DataFrame(Scope_annd.obsm['X_pca'],index=Scope_annd.obs.index)[1] ,\n",
    "        dr_tsne['Y'] ,\n",
    "        dr_umap['Y']\n",
    "    ], sort=False, axis=1, join='outer' )\n",
    "Embeddings_Y.columns = ['1','2','3','4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaJson = {}\n",
    "\n",
    "metaJson['embeddings'] = [\n",
    "    {\n",
    "        \"id\": -1,\n",
    "        \"name\": f\"Scanpy t-SNE (highly variable genes)\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": f\"Scanpy UMAP  (highly variable genes)\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"Scanpy PC1/PC2\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "       \"name\": \"SCENIC AUC t-SNE\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"name\": \"SCENIC AUC UMAP\"\n",
    "    },\n",
    "]\n",
    "\n",
    "metaJson[\"louvain\"] = [{\n",
    "            \"id\": 0,\n",
    "            \"group\": \"Scanpy-louvain\",\n",
    "            \"name\": \"Scanpy louvain default resolution\",\n",
    "            \"clusters\": [],\n",
    "        }]\n",
    "\n",
    "metaJson[\"leiden\"] = [{\n",
    "            \"id\": 5,\n",
    "            \"group\": \"Scanpy-leiden\",\n",
    "            \"name\": \"Scanpy leiden default resolution\",\n",
    "            \"clusters\": [],\n",
    "        }]\n",
    "\n",
    "metaJson[\"metrics\"] = [\n",
    "        {\n",
    "            \"name\": \"nUMI\"\n",
    "        }, {\n",
    "            \"name\": \"nGene\"\n",
    "        }, {\n",
    "            \"name\": \"pct_counts_mt\"\n",
    "        }\n",
    "]\n",
    "\n",
    "metaJson[\"annotations\"] = [\n",
    "\n",
    "    {\n",
    "        \"name\": \"SampleID\",\n",
    "       \"values\": list(set(Scope_annd.obs['SampleID'].values))\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Cell_Type_Sample\",\n",
    "       \"values\": list(set(Scope_annd.obs['Cell_Type_Sample'].values))\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"PAC_anno\",\n",
    "       \"values\": list(set(Scope_annd.obs['PAC_anno'].values))\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"lineage\",\n",
    "       \"values\": list(set(Scope_annd.obs['lineage'].values))\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"lineage_Sample\",\n",
    "       \"values\": list(set(Scope_annd.obs['lineage_Sample'].values))\n",
    "    },\n",
    "     {\n",
    "        \"name\": \"Sample\",\n",
    "       \"values\": list(set(Scope_annd.obs['Sample'].values))\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"Cell_Type\",\n",
    "       \"values\": list(set(Scope_annd.obs['Cell_Type'].values))\n",
    "    },\n",
    "        {\n",
    "       \"name\": \"Cytogenetic_Sample\",\n",
    "       \"values\": list(set(Scope_annd.obs['Cytogenetic_Sample'].values))\n",
    "     },\n",
    "    {\n",
    "        \"name\": \"Cytogenetic\",\n",
    "       \"values\": list(set(Scope_annd.obs['Cytogenetic'].values))\n",
    "    },\n",
    " \n",
    "    {\n",
    "        \"name\": \"Prognosis\",\n",
    "        \"values\": list(set(Scope_annd.obs['Prognosis'].values))\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Relapsed\",\n",
    "        \"values\": list(set(Scope_annd.obs['Relapsed'].values))\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Remission\",\n",
    "        \"values\": list(set(Scope_annd.obs['Remission'].values))\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Age\",\n",
    "        \"values\": list(set(Scope_annd.obs['Age'].values))\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SampleType\",\n",
    "        \"values\": list(set(Scope_annd.obs['SampleType'].values))\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Leiden_clusters_Scanpy\",\n",
    "        \"values\": list(set(Scope_annd.obs['leiden'].values))\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"louvain_clusters_Scanpy\",\n",
    "        \"values\": list(set(Scope_annd.obs['louvain'].values))\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "# SCENIC regulon thresholds:\n",
    "metaJson[\"regulonThresholds\"] = rt\n",
    "\n",
    "for i in range(max(set([int(x) for x in Scope_annd.obs['leiden']])) + 1):\n",
    "    clustDict = {}\n",
    "    clustDict['id'] = i\n",
    "    clustDict['description'] = f'Unannotated Cluster {i + 1}'\n",
    "    metaJson['leiden'][0]['clusters'].append(clustDict)\n",
    "    \n",
    "louvain = pd.DataFrame()\n",
    "louvain[\"0\"] = Scope_annd.obs['louvain'].values.astype(np.int64)\n",
    "   \n",
    "leiden = pd.DataFrame()\n",
    "leiden[\"0\"] = Scope_annd.obs['leiden'].values.astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfToNamedMatrix(df):\n",
    "    arr_ip = [tuple(i) for i in df.values]\n",
    "    dtyp = np.dtype(list(zip(df.dtypes.index, df.dtypes)))\n",
    "    arr = np.array(arr_ip, dtype=dtyp)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_attrs = {\n",
    "    \"CellID\": np.array(Scope_annd.obs.index),\n",
    "    \"nUMI\": np.array(Scope_annd.obs['total_counts'].values),\n",
    "    \"nGene\": np.array(Scope_annd.obs['n_genes'].values),\n",
    "    \"Louvain_clusters_Scanpy\": np.array( Scope_annd.obs['louvain'].values ),\n",
    "    \"Leiden_clusters_Scanpy\": np.array( Scope_annd.obs['leiden'].values ),\n",
    "    \"SampleID\": np.array(Scope_annd.obs['SampleID'].values),\n",
    "    \"Cytogenetic\": np.array(Scope_annd.obs['Cytogenetic'].values),\n",
    "    \"lineage\": np.array(Scope_annd.obs['lineage'].values),\n",
    "    \"Cell_Type\": np.array(Scope_annd.obs['Cell_Type'].values),\n",
    "    \"Cytogenetic_Sample\": np.array(Scope_annd.obs['Cytogenetic_Sample'].values),\n",
    "    \"Sample\":np.array(Scope_annd.obs['Sample'].values),\n",
    "    \"Age\":np.array(Scope_annd.obs['Age'].values),\n",
    "    \"Prognosis\":np.array(Scope_annd.obs['Prognosis'].values),\n",
    "    \"Relapsed\":np.array(Scope_annd.obs['Relapsed'].values),\n",
    "    \"Remission\":np.array(Scope_annd.obs['Remission'].values),\n",
    "    \"SampleType\":np.array(Scope_annd.obs['SampleType'].values),\n",
    "    \"Cell_Type_Sample\":np.array(Scope_annd.obs['Cell_Type_Sample'].values),\n",
    "     \"PAC_anno\":np.array(Scope_annd.obs['PAC_anno'].values),\n",
    "    \"lineage_Sample\":np.array(Scope_annd.obs['lineage_Sample'].values),\n",
    "    \"pct_counts_mt\": np.array(Scope_annd.obs['pct_counts_mt'].values),\n",
    "    \"Embedding\": dfToNamedMatrix(tsneDF),\n",
    "    \"Embeddings_X\": dfToNamedMatrix(Embeddings_X),\n",
    "    \"Embeddings_Y\": dfToNamedMatrix(Embeddings_Y),\n",
    "    \"RegulonsAUC\": dfToNamedMatrix(auc_mtx),\n",
    "    \"leiden\": dfToNamedMatrix(leiden),\n",
    "    \"ClusterID\": np.array(Scope_annd.obs['leiden'].values)\n",
    "}\n",
    "\n",
    "row_attrs = {\n",
    "    \"Gene\": lf.ra.Gene,\n",
    "    \"Regulons\": regulons,\n",
    "}\n",
    "\n",
    "attrs = {\n",
    "    \"title\": \"sampleTitle\",\n",
    "    \"MetaData\": json.dumps(metaJson),\n",
    "    \"Genome\": 'hg38',\n",
    "    \"SCopeTreeL1\": \"\",\n",
    "    \"SCopeTreeL2\": \"\",\n",
    "    \"SCopeTreeL3\": \"\"\n",
    "}\n",
    "\n",
    "# compress the metadata field:\n",
    "attrs['MetaData'] = base64.b64encode(zlib.compress(json.dumps(metaJson).encode('ascii'))).decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.create(\n",
    "    filename = f_final_loom ,\n",
    "    layers=lf[:,:],\n",
    "    row_attrs=row_attrs, \n",
    "    col_attrs=col_attrs, \n",
    "    file_attrs=attrs\n",
    ")\n",
    "lf.close() # close original pyscenic loom file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pySCENIC protocol: PBMC10k downstream analyses\n",
    "August 2019\n",
    "Dataset: 10k PBMCs from a Healthy Donor available from 10x Genomics (here).\n",
    "This notebook uses a loom file generated from the first part of the SCENIC protocol, described in: PBMC10k_SCENIC-protocol-CLI.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import loompy as lp\n",
    "from sklearn.manifold import TSNE\n",
    "import json\n",
    "import base64\n",
    "import zlib\n",
    "from pyscenic.plotting import plot_binarization\n",
    "from pyscenic.export import add_scenic_metadata\n",
    "from pyscenic.cli.utils import load_signatures\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenic output\n",
    "lf = lp.connect(f_final_loom, mode='r', validate=False )\n",
    "meta = json.loads(zlib.decompress(base64.b64decode( lf.attrs.MetaData )))\n",
    "exprMat = pd.DataFrame( lf[:,:], index=lf.ra.Gene, columns=lf.ca.CellID).T\n",
    "auc_mtx = pd.DataFrame( lf.ca.RegulonsAUC, index=lf.ca.CellID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of regulons:\n",
    "regulons = {}\n",
    "for i,r in pd.DataFrame(lf.ra.Regulons,index=lf.ra.Gene).iteritems():\n",
    "    regulons[i] =  list(r[r==1].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell annotations from the loom column attributes:\n",
    "cellAnnot = pd.concat(\n",
    "    [   pd.DataFrame( lf.ca.SampleID, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.Cell_Type, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.Cytogenetic_Sample, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.Cytogenetic, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.lineage, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.Sample, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.Age, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.Prognosis, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.Relapsed, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.Remission, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.SampleType, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( Scope_annd.obs['PAC_anno'], index=Scope_annd.obs.index ),\n",
    "        pd.DataFrame( lf.ca.Cell_Type_Sample, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.lineage_Sample, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.Louvain_clusters_Scanpy, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.Leiden_clusters_Scanpy, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.pct_counts_mt, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.nGene, index=lf.ca.CellID ),\n",
    "        pd.DataFrame( lf.ca.nUMI, index=lf.ca.CellID ),\n",
    "    ],\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellAnnot.columns = [\n",
    " \"SampleID\",\n",
    " \"Cell_Type\",\n",
    " \"Cytogenetic_Sample\",\n",
    " \"Cytogenetic\",\n",
    " \"lineage\",\n",
    " \"Sample\",\n",
    " \"Age\",\n",
    " \"Prognosis\",\n",
    " \"Relapsed\",\n",
    " \"Remission\",\n",
    " \"SampleType\",\n",
    " \"PAC_anno\",\n",
    " \"Cell_Type_Sample\",\n",
    " \"lineage_Sample\",\n",
    " 'Louvain_clusters_Scanpy',\n",
    " 'Leiden_clusters_Scanpy',\n",
    " 'pct_counts_mt',\n",
    " 'nGene',\n",
    " 'nUMI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture embeddings:\n",
    "dr = [\n",
    "    pd.DataFrame( lf.ca.Embedding, index=lf.ca.CellID )\n",
    "]\n",
    "dr_names = [\n",
    "    meta['embeddings'][0]['name'].replace(\" \",\"_\")\n",
    "]\n",
    "\n",
    "# add other embeddings\n",
    "drx = pd.DataFrame( lf.ca.Embeddings_X, index=lf.ca.CellID )\n",
    "dry = pd.DataFrame( lf.ca.Embeddings_Y, index=lf.ca.CellID )\n",
    "\n",
    "for i in range( len(drx.columns) ):\n",
    "    dr.append( pd.concat( [ drx.iloc[:,i], dry.iloc[:,i] ], sort=False, axis=1, join='outer' ))\n",
    "    dr_names.append( meta['embeddings'][i+1]['name'].replace(\" \",\"_\").replace('/','-') )\n",
    "\n",
    "# rename columns:\n",
    "for i,x in enumerate( dr ):\n",
    "    x.columns = ['X','Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display a motifs table with motif logos\n",
    "View the motifs table along with motif logos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions (not yet integrated into pySCENIC):\n",
    "\n",
    "from pyscenic.utils import load_motifs\n",
    "import operator as op\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "BASE_URL = \"http://motifcollections.aertslab.org/v9/logos/\"\n",
    "COLUMN_NAME_LOGO = \"MotifLogo\"\n",
    "COLUMN_NAME_MOTIF_ID = \"MotifID\"\n",
    "COLUMN_NAME_TARGETS = \"TargetGenes\"\n",
    "\n",
    "def display_logos(df: pd.DataFrame, top_target_genes: int = 3, base_url: str = BASE_URL):\n",
    "    \"\"\"\n",
    "    :param df:\n",
    "    :param base_url:\n",
    "    \"\"\"\n",
    "    # Make sure the original dataframe is not altered.\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Add column with URLs to sequence logo.\n",
    "    def create_url(motif_id):\n",
    "        return '<img src=\"{}{}.png\" style=\"max-height:124px;\"></img>'.format(base_url, motif_id)\n",
    "    df[(\"Enrichment\", COLUMN_NAME_LOGO)] = list(map(create_url, df.index.get_level_values(COLUMN_NAME_MOTIF_ID)))\n",
    "    \n",
    "    # Truncate TargetGenes.\n",
    "    def truncate(col_val):\n",
    "        return sorted(col_val, key=op.itemgetter(1))[:top_target_genes]\n",
    "    df[(\"Enrichment\", COLUMN_NAME_TARGETS)] = list(map(truncate, df[(\"Enrichment\", COLUMN_NAME_TARGETS)]))\n",
    "    \n",
    "    MAX_COL_WIDTH = pd.get_option('display.max_colwidth')\n",
    "    pd.set_option('display.max_colwidth', 200)\n",
    "    display(HTML(df.head().to_html(escape=False)))\n",
    "    pd.set_option('display.max_colwidth', MAX_COL_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_motifs = load_motifs('/oak/stanford/groups/cgawad/home/Cancer_Studies/SC_RNA_SEQ/ALSF_AML/scanpy/reg-py_LSC_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_motifs = ['HOXA13','GATA1']\n",
    "df_motifs_sel = df_motifs.iloc[ [ True if x in selected_motifs else False for x in df_motifs.index.get_level_values('TF') ] ,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_logos(df_motifs.head())\n",
    "display_logos( df_motifs_sel.sort_values([('Enrichment','NES')], \n",
    "                                         ascending=False).head(9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction plots\n",
    "Show both highly variable genes and SCENIC UMAP plots with Louvain clustering and cell type classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cytoolz import compose\n",
    "from pyscenic.transform import df2regulons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_regulons(motifs, db_names=('hg38.refseq-r80.10kb.up.and.down.tss.mc9nr')):\n",
    "    motifs.columns = motifs.columns.droplevel(0)\n",
    "\n",
    "    def contains(*elems):\n",
    "        def f(context):\n",
    "            return any(elem in context for elem in elems)\n",
    "        return f\n",
    "\n",
    " # For the creation of regulons we only keep the 10-species databases and the activating modules. We also remove the\n",
    " # enriched motifs for the modules that were created using the method 'weight>50.0%' (because these modules are not part\n",
    "# of the default settings of modules_from_adjacencies anymore.\n",
    "    motifs = motifs[\n",
    "        np.fromiter(map(compose(op.not_, contains('weight>50.0%')), motifs.Context), dtype=np.bool) & \\\n",
    "       # np.fromiter(map(contains(*db_names), motifs.Context), dtype=np.bool) & \\\n",
    "        np.fromiter(map(contains('activating'), motifs.Context), dtype=np.bool)]\n",
    "\n",
    "    # We build regulons only using enriched motifs with a NES of 3.0 or higher; we take only directly annotated TFs or TF annotated\n",
    "    # for an orthologous gene into account; and we only keep regulons with at least 10 genes.\n",
    "    regulons = list(filter(lambda r: len(r) >= 10, df2regulons(motifs[(motifs['NES'] >= 3.0) \n",
    "                                                                      & ((motifs['Annotation'] == 'gene is directly annotated')\n",
    "                                                                        | (motifs['Annotation'].str.startswith('gene is orthologous to')\n",
    "                                                                           & motifs['Annotation'].str.endswith('which is directly annotated for motif')))\n",
    "                                                                     ])))\n",
    "    \n",
    "    # Rename regulons, i.e. remove suffix.\n",
    "    return list(map(lambda r: r.rename(r.transcription_factor), regulons))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulons = derive_regulons(df_motifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_logo(regulon, base_url = BASE_URL):\n",
    "    for elem in regulon.context:\n",
    "        if elem.endswith('.png'):\n",
    "            return '<img src=\"{}{}\" style=\"max-height:124px;\"></img>'.format(base_url, elem)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL='http://motifcollections.aertslab.org/v10/logos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regulons = pd.DataFrame(data=[list(map(op.attrgetter('name'), regulons)),\n",
    "                                 list(map(len, regulons)),\n",
    "                                 list(map(fetch_logo, regulons))], \n",
    "                                 index=['name', 'count', 'logo']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regulons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_COL_WIDTH = pd.get_option('display.max_colwidth')\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "display(HTML(df_regulons.to_html(escape=False)))\n",
    "pd.set_option('display.max_colwidth', MAX_COL_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regulons.to_csv('AML_regulons_image.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd.obs = Scope_annd.obs[Scope_annd.obs.columns.drop(list(Scope_annd.obs.filter(regex='Regulon')))]\n",
    "Scope_annd.var = Scope_annd.var[Scope_annd.var.columns.drop(list(Scope_annd.var.filter(regex='Regulon')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scope_annd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_scenic_metadata(Scope_annd, auc_mtx, regulons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML=Scope_annd[(Scope_annd.obs[\"lineage\"].isin(['AML'])) |\n",
    "               (Scope_annd.obs[\"SampleType\"].isin(['HealthyBM']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tcell=Scope_annd[(Scope_annd.obs[\"lineage\"].isin(['AML-T','T','AML-NK','NK']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HBM=Scope_annd[Scope_annd.obs[\"SampleType\"].isin(['HealthyBM'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HBM=HBM[np.logical_not(HBM.obs[\"lineage_Sample\"].isin(['AML','AML-B',\n",
    "                                                                    'AML-Ery',\n",
    "                                                                    'AML-Mono',\n",
    "                                                                    'AML-T',\n",
    "                                                                    'AML-NK' ]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML_2=AML[np.logical_not(AML.obs[\"lineage_Sample\"].isin(['AML','AML-B',\n",
    "                                                                    'AML-Ery',\n",
    "                                                                    'AML-Mono',\n",
    "                                                                    'AML-T',\n",
    "                                                                    'AML-NK',\n",
    "                                                         'PlasmaB',\n",
    "                                                         #'B',\n",
    "                                                         #'T',\n",
    "                                                         #'NK',\n",
    "                                                         #'Monocyte',\n",
    "                                                         #'Erythrocytes',\n",
    "                                                         #'Myeloid Pro'\n",
    "                                                                    ]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML_3=AML_2[(AML_2.obs[\"PAC_anno\"].isin(['0_HSPC',\n",
    "                                          'Myeloid_Pro',\n",
    "                                          'FPAC_1',\n",
    "                                          'FPAC_2',\n",
    "                                          'FPAC_3',\n",
    "                                          'FPAC_4',\n",
    "                                        \n",
    "                                          'PPAC_1',\n",
    "                                          'PPAC_2',\n",
    "                                          'PPAC_3',\n",
    "                                          'PPAC_4',\n",
    "                                        \n",
    "                                          'AML',\n",
    "                                         'AML-PCNA',\n",
    "                                         'AML-MKI67'\n",
    "                                         ]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML_4=AML_2[(AML_2.obs[\"PAC_anno\"].isin(['0_HSPC',\n",
    "                                          'Myeloid_Pro',\n",
    "                                          'FPAC_1',\n",
    "                                          'FPAC_2',\n",
    "                                          'FPAC_3',\n",
    "                                          'FPAC_4',\n",
    "                                       \n",
    "                                          'PPAC_1',\n",
    "                                          'PPAC_2',\n",
    "                                          'PPAC_3',\n",
    "                                          'PPAC_4',\n",
    "                                        \n",
    "                                           #'CD34+ProB',\n",
    "                                          #'CD14_Monocyte'\n",
    "                                         ]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name=\"PAC_anno\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs =AML_2.obs\n",
    "signature_column_names = list(df_obs.select_dtypes('number').columns)\n",
    "signature_column_names = list(filter(lambda s: s.startswith('Regulon('), signature_column_names))\n",
    "df_scores = df_obs[signature_column_names + [column_name]]\n",
    "df_results = ((df_scores.groupby(by=column_name).mean() - df_obs[signature_column_names].mean())/ df_obs[signature_column_names].std()).stack().reset_index().rename(columns={'level_1': 'regulon', 0:'Z'})\n",
    "df_results['regulon'] = list(map(lambda s: s[8:-1], df_results.regulon))\n",
    "df_results[(df_results.Z >= 0.0)].sort_values('Z', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df_results.pivot(index=column_name,columns='regulon',values='Z')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Group_list=df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topreg = []\n",
    "for i,c in enumerate(Group_list):\n",
    "    topreg.extend(\n",
    "        list(df.T[c].sort_values(ascending=False)[:5].index)\n",
    "    )\n",
    "topreg = list(set(topreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= df_results[df_results.regulon.isin(topreg)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2= df.loc[:, df.columns.isin(topreg)]\n",
    "df_2.to_csv('ALSF_HBM_enriched_%s_Aucell_Z_top5_Heatmap_new_results.csv'%column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3=df_2.T\n",
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_color_df=AML_2.obs[['PAC_anno']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (row_color_df['PAC_anno'] =='0_HSPC'),\n",
    "    (row_color_df['PAC_anno'] == 'Myeloid_Pro'),\n",
    "    (row_color_df['PAC_anno'].isin(['PPAC_1','PPAC_2','PPAC_3','PPAC_4'])),\n",
    "    (row_color_df['PAC_anno'].isin(['FPAC_1','FPAC_2','FPAC_3','FPAC_4'])),\n",
    "    (row_color_df['PAC_anno'].isin(['0_HSPC',\n",
    "                                    'Myeloid_Pro',\n",
    "                                    'PPAC_1','PPAC_2','PPAC_3','PPAC_4',\n",
    "                                    'FPAC_1','FPAC_2','FPAC_3','FPAC_4'])==False),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ['#ffff00', '#e377c2', '#f7aa58','#72bcd5','#C7C7C7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_color_df['PAC_anno_color'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_color=row_color_df[['PAC_anno','PAC_anno_color']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_color=row_color.set_index('PAC_anno')\n",
    "row_color = row_color.rename(columns={'PAC_anno_color': 'PAC_anno'}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "g = sns.clustermap(df_2, annot=False,  square=True,  linecolor='gray',\n",
    "    yticklabels=True, xticklabels=True, \n",
    "                   vmin=0, \n",
    "                   vmax=4, \n",
    "    method='complete',metric=\"correlation\",\n",
    "    cmap=\"coolwarm\",tree_kws=dict(linewidths=2),\n",
    "    figsize=(45,15),\n",
    "        row_colors=row_color,colors_ratio=0.01)\n",
    "g.cax.set_visible(True)\n",
    "g.ax_heatmap.set_ylabel('')\n",
    "g.ax_heatmap.set_xlabel('')\n",
    "g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_yticklabels(), rotation=0) # For y axis\n",
    "plt.savefig('ALSF_HBM_enriched_%s_Aucell_Z_top5_Heatmap_new_results.pdf'%column_name,\n",
    "            dpi=300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.set_figure_params(frameon=False, dpi=150, fontsize=14)\n",
    "sc.pl.umap(Combo, color=['PAC_anno'],\n",
    "           save='_ALSF_AML-regulon_PAC_anno.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.set_figure_params(frameon=False, dpi=150, fontsize=14)\n",
    "sc.pl.umap(Combo, color=[                            \n",
    "'Regulon(HOXA10_(+))','Regulon(HOXA7_(+))','Regulon(HOXA9_(+))',\n",
    "'Regulon(MYB_(+))','Regulon(REST_(+))','Regulon(SOX4_(+))',\n",
    "'Regulon(SHOX2_(+))','Regulon(HMX3_(+))','Regulon(PITX1_(+))',\n",
    "],cmap='bwr',ncols=3,\n",
    "           save='_ALSF_AML-rss_regulon.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_color_df=AML_2.obs[['Cell_Type_Sample','Prognosis','Relapsed']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (row_color_df['Prognosis'] =='0_HealthyBM'),\n",
    "    (row_color_df['Prognosis'] == 'Deceased'),\n",
    "    (row_color_df['Prognosis'] == 'Alive'),\n",
    "    (row_color_df['Prognosis'] == 'none')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"#e76254\", \"#ef8a47\", \"#f7aa58\", \"#ffd06f\", \"#ffe6b7\", \"#aadce0\", \"#72bcd5\", \"#528fad\", \"#376795\", \"#1e466e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ['#aadce0', '#e76254', '#f7aa58','#C7C7C7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_color_df['Prognosis_color'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (row_color_df['Relapsed'] =='0_HealthyBM'),\n",
    "    (row_color_df['Relapsed'] == 'True'),\n",
    "    (row_color_df['Relapsed'] == 'False'),\n",
    "      (row_color_df['Relapsed'] == 'none')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ['#aadce0', '#e76254', '#f7aa58','#C7C7C7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_color_df['Relapsed_color'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_color=row_color_df[['Cell_Type_Sample','Prognosis_color','Relapsed_color']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_color=row_color.set_index('Cell_Type_Sample')\n",
    "row_color = row_color.rename(columns={'Prognosis_color': 'Prognosis', 'Relapsed_color': 'Relapsed'}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "g = sns.clustermap(df_2, annot=False,  square=True,  linecolor='gray',\n",
    "    yticklabels=True, xticklabels=True, \n",
    "                   vmin=0, \n",
    "                   vmax=4, \n",
    "    method='ward',metric=\"correlation\",\n",
    "    cmap=\"coolwarm\",tree_kws=dict(linewidths=2),\n",
    "    figsize=(35,15),\n",
    "    row_colors=row_color,colors_ratio=0.01)\n",
    "g.cax.set_visible(True)\n",
    "g.ax_heatmap.set_ylabel('')\n",
    "g.ax_heatmap.set_xlabel('')\n",
    "g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_yticklabels(), rotation=0) # For y axis\n",
    "plt.savefig('ALSF_LSC_%s_Aucell_Z_top3_Heatmap_new_results.pdf'%column_name,\n",
    "            dpi=600, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def palplot(pal, names, colors=None, size=1):\n",
    "    n = len(pal)\n",
    "    f, ax = plt.subplots(1, 1, figsize=(n * size, size))\n",
    "    ax.imshow(np.arange(n).reshape(1, n),\n",
    "              cmap=mpl.colors.ListedColormap(list(pal)),\n",
    "              interpolation=\"nearest\", aspect=\"auto\")\n",
    "    ax.set_xticks(np.arange(n) - .5)\n",
    "    ax.set_yticks([-.5, .5])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    colors = n * ['k'] if colors is None else colors\n",
    "    for idx, (name, color) in enumerate(zip(names, colors)):\n",
    "        ax.text(0.0+idx, 0.0, name, color=color, horizontalalignment='center', verticalalignment='top')\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ['#ffff00', '#e377c2', '#f7aa58','#72bcd5','#C7C7C7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=['HSPC', 'Myeloid Pro', 'FPACs','PPACs','Others']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.font_manager.get_font_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set(font_scale=0.45)\n",
    "fig = palplot(values,label, size=0.5,weight='bold')\n",
    "\n",
    "plt.savefig(\"AML_%s_PAC_heatmap-legend-Prognosis.pdf\"% anno, dpi=300,\n",
    "            bbox_inches = \"tight\"\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regulon specificity scores (RSS) across predicted cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyscenic.rss import regulon_specificity_scores\n",
    "from pyscenic.plotting import plot_rss\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "import seaborn as sns\n",
    "from pyscenic.binarization import binarize\n",
    "import fastcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scope_annd = sc.read_h5ad(f_anndata_path)\n",
    "#Scope_annd.uns['log1p'][\"base\"] = None\n",
    "#Scope_annd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML=Scope_annd[(Scope_annd.obs[\"lineage\"].isin(['AML'])) | (Scope_annd.obs[\"SampleType\"].isin(['HealthyBM']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML_2=AML[np.logical_not(AML.obs[\"lineage_Sample\"].isin(['AML','AML-B',\n",
    "                                                                    'AML-Ery',\n",
    "                                                                    'AML-Mono',\n",
    "                                                                    'AML-T',\n",
    "                                                                    'AML-NK',\n",
    "                                                         'B','PlasmaB',\n",
    "                                                         'T',\n",
    "                                                         'NK',\n",
    "                                                                    ]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellAnnot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HBMAnnot=cellAnnot[cellAnnot[\"SampleType\"].isin(['HealthyBM'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HBMAnnot=HBMAnnot[HBMAnnot[\"Cell_Type\"].isin([\n",
    " 'Activated CD4T',\n",
    " 'CD20+B',\n",
    " 'CD34+ProB',\n",
    " 'CTL',\n",
    " 'Erythrocytes',\n",
    " 'HSPC',\n",
    " 'Myeloid Pro',\n",
    " 'NK',\n",
    " 'Naive CD4T',\n",
    " 'Naive CD8T',\n",
    " 'PreB',\n",
    " 'ProB',\n",
    " 'mDC',\n",
    " 'pDC']              \n",
    "                                                                  )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMLAnnot=cellAnnot[(cellAnnot[\"lineage\"].isin(['AML'])) | (cellAnnot[\"SampleType\"].isin(['HealthyBM']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMLAnnot=AMLAnnot[np.logical_not(AMLAnnot[\"lineage_Sample\"].isin(['AML','AML-B',\n",
    "                                                                    'AML-Ery',\n",
    "                                                                    'AML-Mono',\n",
    "                                                                    'AML-T',\n",
    "                                                                  'AML-NK',\n",
    "                                                                  # 'T',\n",
    "                                                                  #'NK',\n",
    "                                                                  #'B',\n",
    "                                                                  'PlasmaB',\n",
    "                                                                #'Monocyte',\n",
    "                                                                #'Erythrocytes'\n",
    "                                                                 ]\n",
    "                                                                  ))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AML_auc_mtx=auc_mtx[auc_mtx.index.isin(AMLAnnot.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno='Sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_cellType = regulon_specificity_scores(AML_auc_mtx, AMLAnnot[anno] )\n",
    "rss_cellType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rss_cellType.to_csv('AML_RSS_LSC_by_sample_Heatmap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_cellType.to_csv('AML_RSS_LSC_enriched_by_sample_%s.csv'%anno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSS panel plot with all cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats =list(set(AMLAnnot[anno]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_cellType.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50,60))\n",
    "for c,num in zip(cats, range(1,len(cats)+1)):\n",
    "    x=rss_cellType.T[c]\n",
    "    ax = fig.add_subplot(6,6,num)\n",
    "    plot_rss(rss_cellType, c, top_n=10, max_n=None, ax=ax)\n",
    "    ax.set_ylim( x.min()-(x.max()-x.min())*0.05 , x.max()+(x.max()-x.min())*0.05 )\n",
    "    for t in ax.texts:\n",
    "        t.set_fontsize(18)\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('')\n",
    "    adjust_text(ax.texts, autoalign='xy', ha='right', va='bottom', arrowprops=dict(arrowstyle='-',color='lightgrey'), precision=0.001 )\n",
    " \n",
    "fig.text(0.5, 0.0, 'Regulon', ha='center', va='center', size='x-large')\n",
    "fig.text(0.00, 0.5, 'Regulon specificity score (RSS)', ha='center', va='center', rotation='vertical', size='x-large')\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({\n",
    "    'figure.autolayout': True,\n",
    "        'figure.titlesize': 'large' ,\n",
    "        'axes.labelsize': 'medium',\n",
    "        'axes.titlesize':'large',\n",
    "        'xtick.labelsize':'medium',\n",
    "        'ytick.labelsize':'medium'\n",
    "        })\n",
    "plt.savefig('AML_RSS_LSC_enriched_by_sample_Top10_LSC_Cell_Type_2_new_result.pdf', dpi=600, bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the top 5 regulons from each cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topreg = []\n",
    "for i,c in enumerate(cats):\n",
    "    topreg.extend(\n",
    "        list(rss_cellType.T[c].sort_values(ascending=False)[:3].index)\n",
    "    )\n",
    "topreg = list(set(topreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topreg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx=AML_auc_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
      "/tmp/ipykernel_184089/3999204926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n"
     ]
    }
   ],
   "source": [
    "mtx_Z = pd.DataFrame( index=mtx.index )\n",
    "for col in list(mtx.columns):\n",
    "    mtx_Z[ col ] = (mtx[col] - mtx[col].mean()) / mtx[col].std(ddof=0)\n",
    "#auc_mtx_Z.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30 color set\n",
    "mycolor =[ '#5D4C86','#7DFC00', '#FCFF5D','#3750DB',\n",
    "            '#FFC413',  '#235B54','#632819','#228C68',\n",
    "          '#F22020','#E68F66','#F07CAB','#D30B94',\n",
    "       '#772B9D',   '#0EC434','#8AB8E8',\n",
    "           '#29BDAB','#3998F5',\n",
    "           '#37294F','#277DA7','#FFCBA5','#C56133',\n",
    "            '#EDEFF3','#C3A5B4','#946AA2',\n",
    "          '#B732CC','#991919',  '#676790',\n",
    "        \n",
    "        \n",
    "       \n",
    "      \n",
    "       '#96341C','#F47a22','#737388', '#2F2aa0',\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def palplot(pal, names, colors=None, size=1):\n",
    "    n = len(pal)\n",
    "    f, ax = plt.subplots(1, 1, figsize=(n * size, size))\n",
    "    ax.imshow(np.arange(n).reshape(1, n),\n",
    "              cmap=mpl.colors.ListedColormap(list(pal)),\n",
    "              interpolation=\"nearest\", aspect=\"auto\")\n",
    "    ax.set_xticks(np.arange(n) - .5)\n",
    "    ax.set_yticks([-.5, .5])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    colors = n * ['k'] if colors is None else colors\n",
    "    for idx, (name, color) in enumerate(zip(names, colors)):\n",
    "        ax.text(0.0+idx, 0.0, name, color=color, horizontalalignment='center', verticalalignment='center')\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cats = sorted(list(set(AMLAnnot['Cytogenetic'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette(mycolor,n_colors=len(cats) )\n",
    "colorsd = dict( zip( cats, colors ))\n",
    "colormap = [ colorsd[x] for x in AMLAnnot[anno] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set(font_scale=0.55)\n",
    "fig = palplot(colors, cats, size=0.8)\n",
    "plt.savefig(\"AML_%s_LSC_heatmap-legend-top10.pdf\"% anno, dpi=600,\n",
    "            #bbox_inches = \"tight\"\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "g = sns.clustermap(mtx_Z[topreg], annot=False,  square=False,  linecolor='gray',\n",
    "    yticklabels=False, xticklabels=True, vmin=-2, vmax=6, row_colors=colormap,\n",
    "    cmap=\"YlGnBu\", figsize=(21,25) )\n",
    "g.cax.set_visible(True)\n",
    "g.ax_heatmap.set_ylabel('')\n",
    "g.ax_heatmap.set_xlabel('')\n",
    "plt.savefig(\"AML_%s_LSC_enriched-heatmap-top10.pdf\"% anno, \n",
    "            dpi=600, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Generate a binary regulon activity matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_mtx, auc_thresholds = binarize(mtx, num_workers=25 )\n",
    "binary_mtx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select regulons:\n",
    "r = [ 'ELF4_(+)', 'ELK3_(+)', 'IKZF2_(+)', 'IRF5_(+)', \n",
    "     #'PRDM16_(+)', 'SOX4_(+)','SPI1_(+)', 'TAGLN2_(+)' \n",
    "    ]\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 4), dpi=150, sharey=False)\n",
    "for i,ax in enumerate(axs):\n",
    "    sns.distplot(AML_auc_mtx[ r[i] ], ax=ax, norm_hist=True, bins=100)\n",
    "    ax.plot( [ auc_thresholds[ r[i] ] ]*2, ax.get_ylim(), 'r:')\n",
    "    ax.title.set_text( r[i] )\n",
    "    ax.set_xlabel('')\n",
    "    \n",
    "fig.text(-0.01, 0.5, 'Frequency', ha='center', va='center', rotation='vertical', size='large')\n",
    "fig.text(0.5, -0.01, 'AUC', ha='center', va='center', rotation='horizontal', size='large')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('AML_binaryPlot2.pdf', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regulon specificity scores (RSS) across Louvain clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_leiden = regulon_specificity_scores( auc_mtx, cellAnnot['Leiden_clusters_Scanpy'] )\n",
    "rss_leiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " auc_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = sorted( list(set(cellAnnot['Leiden_clusters_Scanpy'])), key=int )\n",
    "\n",
    "fig = plt.figure(figsize=(15, 50))\n",
    "for c,num in zip(cats, range(1,len(cats)+1)):\n",
    "    x=rss_leiden.T[c]\n",
    "    ax = fig.add_subplot(12,5,num)\n",
    "    plot_rss(rss_leiden, c, top_n=5, max_n=None, ax=ax)\n",
    "    ax.set_ylim( x.min()-(x.max()-x.min())*0.05 , x.max()+(x.max()-x.min())*0.05 )\n",
    "    for t in ax.texts:\n",
    "        t.set_fontsize(12)\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('')\n",
    "    adjust_text(ax.texts, autoalign='xy', ha='right', va='bottom', arrowprops=dict(arrowstyle='-',color='lightgrey'), precision=0.001 )\n",
    " \n",
    "fig.text(0.5, 0.0, 'Regulon', ha='center', va='center', size='x-large')\n",
    "fig.text(0.00, 0.5, 'Regulon specificity score (RSS)', ha='center', va='center', rotation='vertical', size='x-large')\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({\n",
    "    'figure.autolayout': True,\n",
    "        'figure.titlesize': 'large' ,\n",
    "        'axes.labelsize': 'medium',\n",
    "        'axes.titlesize':'large',\n",
    "        'xtick.labelsize':'medium',\n",
    "        'ytick.labelsize':'medium'\n",
    "        })\n",
    "plt.savefig(\"AML_Leiden-RSS-top5.png\", dpi=150, bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topreg = []\n",
    "for i,c in enumerate(cats):\n",
    "    topreg.extend(\n",
    "        list(rss_louvain.T[c].sort_values(ascending=False)[:5].index)\n",
    "    )\n",
    "topreg = list(set(topreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def palplot(pal, names, colors=None, size=1):\n",
    "    n = len(pal)\n",
    "    f, ax = plt.subplots(1, 1, figsize=(n * size, size))\n",
    "    ax.imshow(np.arange(n).reshape(1, n),\n",
    "              cmap=mpl.colors.ListedColormap(list(pal)),\n",
    "              interpolation=\"nearest\", aspect=\"auto\")\n",
    "    ax.set_xticks(np.arange(n) - .5)\n",
    "    ax.set_yticks([-.5, .5])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    colors = n * ['k'] if colors is None else colors\n",
    "    for idx, (name, color) in enumerate(zip(names, colors)):\n",
    "        ax.text(0.0+idx, 0.0, name, color=color, horizontalalignment='center', verticalalignment='center')\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette(mycolor,n_colors=len(cats) )\n",
    "colorsd = dict( zip( cats, colors ))\n",
    "colormap = [ colorsd[x] for x in cellAnnot['Leiden_clusters_Scanpy'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set(font_scale=0.8)\n",
    "fig = palplot( colors, cats, size=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "g = sns.clustermap(auc_mtx_Z[topreg], annot=False,  square=False,  linecolor='gray',\n",
    "    yticklabels=False, vmin=-2, vmax=6, row_colors=colormap,\n",
    "    cmap=\"YlGnBu\", figsize=(21,16) )\n",
    "g.cax.set_visible(True)\n",
    "g.ax_heatmap.set_ylabel('')    \n",
    "g.ax_heatmap.set_xlabel('')\n",
    "plt.savefig(\"AML_Leiden_clusters_heatmap-top5.pdf\", dpi=600, bbox_inches = \"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "585px",
    "left": "500px",
    "top": "110px",
    "width": "228px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
